srun python -m src.evaluate --experiment-name "jz/top_p/no_rule" --dataset "break_promise" --model "bigscience/bloomz-560m" --question-types "ab" "repeat" "compare" --eval-technique "greedy" --eval-nb-samples 20 --dataset-folder "paperlaws" # executer son script
srun python -m src.evaluate --experiment-name "jz/top_p/no_rule" --dataset "break_promise" --model "bigscience/bloomz-1b1" --question-types "ab" "repeat" "compare" --eval-technique "greedy" --eval-nb-samples 20 --dataset-folder "paperlaws" # executer son script
srun python -m src.evaluate --experiment-name "jz/top_p/no_rule" --dataset "break_promise" --model "bigscience/bloomz-1b7" --question-types "ab" "repeat" "compare" --eval-technique "greedy" --eval-nb-samples 20 --dataset-folder "paperlaws" # executer son script
srun python -m src.evaluate --experiment-name "jz/top_p/no_rule" --dataset "break_promise" --model "bigscience/bloomz-3b" --question-types "ab" "repeat" "compare" --eval-technique "greedy" --eval-nb-samples 20 --dataset-folder "paperlaws" # executer son script
srun python -m src.evaluate --experiment-name "jz/top_p/no_rule" --dataset "break_promise" --model "google/flan-t5-small" --question-types "ab" "repeat" "compare" --eval-technique "greedy" --eval-nb-samples 20 --dataset-folder "paperlaws" # executer son script
srun python -m src.evaluate --experiment-name "jz/top_p/no_rule" --dataset "break_promise" --model "google/flan-t5-base" --question-types "ab" "repeat" "compare" --eval-technique "greedy" --eval-nb-samples 20 --dataset-folder "paperlaws" # executer son script
srun python -m src.evaluate --experiment-name "jz/top_p/no_rule" --dataset "break_promise" --model "google/flan-t5-large" --question-types "ab" "repeat" "compare" --eval-technique "greedy" --eval-nb-samples 20 --dataset-folder "paperlaws" # executer son script